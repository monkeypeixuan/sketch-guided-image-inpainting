{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Predefined Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torch import autocast\n",
    "from pytorch_lightning import seed_everything\n",
    "import cv2\n",
    "import time\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_img(path, SCALE, pad=False, seg=False, target_size=None):\n",
    "    if seg:\n",
    "        # Load the input image and segmentation map\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        seg_map = Image.open(seg).convert(\"1\")\n",
    "\n",
    "        # Get the width and height of the original image\n",
    "        w, h = image.size\n",
    "\n",
    "        # Calculate the aspect ratio of the original image\n",
    "        aspect_ratio = h / w\n",
    "\n",
    "        # Determine the new dimensions for resizing the image while maintaining aspect ratio\n",
    "        if aspect_ratio > 1:\n",
    "            new_w = int(SCALE * 256 / aspect_ratio)\n",
    "            new_h = int(SCALE * 256)\n",
    "        else:\n",
    "            new_w = int(SCALE * 256)\n",
    "            new_h = int(SCALE * 256 * aspect_ratio)\n",
    "\n",
    "        # Resize the image and the segmentation map to the new dimensions\n",
    "        image_resize = image.resize((new_w, new_h))\n",
    "        segmentation_map_resize = cv2.resize(np.array(seg_map).astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Pad the segmentation map to match the target size\n",
    "        padded_segmentation_map = np.zeros((target_size[1], target_size[0]))\n",
    "        start_x = (target_size[1] - segmentation_map_resize.shape[0]) // 2\n",
    "        start_y = (target_size[0] - segmentation_map_resize.shape[1]) // 2\n",
    "        padded_segmentation_map[start_x: start_x + segmentation_map_resize.shape[0], start_y: start_y + segmentation_map_resize.shape[1]] = segmentation_map_resize\n",
    "\n",
    "        # Create a new RGB image with the target size and place the resized image in the center\n",
    "        padded_image = Image.new(\"RGB\", target_size)\n",
    "        start_x = (target_size[0] - image_resize.width) // 2\n",
    "        start_y = (target_size[1] - image_resize.height) // 2\n",
    "        padded_image.paste(image_resize, (start_x, start_y))\n",
    "\n",
    "        # Update the variable \"image\" to contain the final padded image\n",
    "        image = padded_image\n",
    "\n",
    "    else:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        w, h = image.size        \n",
    "        print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "        w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n",
    "        w = h = 512\n",
    "        image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "        \n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    \n",
    "    if pad or seg:\n",
    "        return 2. * image - 1., new_w, new_h, padded_segmentation_map\n",
    "    \n",
    "    return 2. * image - 1., w, h \n",
    "\n",
    "\n",
    "def load_model_and_get_prompt_embedding(model, opt, device, prompts, inv=False):\n",
    "               \n",
    "    if inv:\n",
    "        inv_emb = model.get_learned_conditioning(prompts, inv)\n",
    "        c = uc = inv_emb\n",
    "    else:\n",
    "        inv_emb = None\n",
    "        \n",
    "    if opt[\"scale\"] != 1.0:\n",
    "        uc = model.get_learned_conditioning(opt[\"n_samples\"] * [\"\"])\n",
    "    else:\n",
    "        uc = None\n",
    "    c = model.get_learned_conditioning(prompts)\n",
    "            \n",
    "    return c, uc, inv_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Setting for Checkpoint and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    \"cross_domain\": False,\n",
    "    \"seed\": 3407,\n",
    "    \"ckpt\": \"./ckpt/v2-1_512-ema-pruned.ckpt\",\n",
    "    \"config\": \"../configs/stable-diffusion/v2-inference.yaml\",\n",
    "    \"scale\": 2.5,\n",
    "    \"n_samples\": 1,\n",
    "    \"f\": 16,\n",
    "    \"C\": 4,\n",
    "    \"ddim_eta\": 0.0,\n",
    "    \"ddim_steps\": 20,\n",
    "    \"outdir\": \"../outputs/\",\n",
    "    \"tau_a\": 0.4,\n",
    "    \"tau_b\": 0.8,\n",
    "}\n",
    "\n",
    "config = OmegaConf.load(opt[\"config\"])\n",
    "model = load_model_from_config(config, opt[\"ckpt\"])    \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "sampler = DPMSolverSampler(model)\n",
    "outpath = opt[\"outdir\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Setting for Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt[\"init_img\"] = \"<path/to/background/>\"\n",
    "opt[\"ref_img\"] = \"<path/to/foreground/>\"\n",
    "opt[\"seg\"] = \"<path/to/segmentation map for foreground/>\"\n",
    "opt[\"prompt\"] = \"your prompt here\"\n",
    "opt[\"ddim_steps\"] = 20\n",
    "\n",
    "# the object and background are from different domains\n",
    "opt[\"cross_domain\"] = True \n",
    "\n",
    "if opt[\"cross_domain\"]:\n",
    "    opt[\"scale\"] = 5.0\n",
    "    file_name = \"cross_domain\"\n",
    "else:\n",
    "    opt[\"scale\"] = 2.5\n",
    "    file_name = \"same_domain\"\n",
    "\n",
    "ORDER = 2\n",
    "\n",
    "# indicating the composition location (a replacement method for the user mask); location and size\n",
    "CENTER_ROW_FROM_TOP = 0.72\n",
    "CENTER_COL_FROM_LEFT = 0.78\n",
    "SCALE = 0.65\n",
    "\n",
    "seed_everything(opt[\"seed\"])\n",
    "batch_size = opt[\"n_samples\"]\n",
    "prompt = opt[\"prompt\"]\n",
    "data = [batch_size * [prompt]]\n",
    "\n",
    "sample_path = os.path.join(outpath, file_name)\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "precision_scope = autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Background and Foreground Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(opt[\"init_img\"])\n",
    "\n",
    "# read background image\n",
    "init_image, target_width, target_height = load_img(opt[\"init_img\"], SCALE)\n",
    "init_image = repeat(init_image.to(device), '1 ... -> b ...', b=batch_size)\n",
    "save_image = init_image.clone()\n",
    "\n",
    "# read foreground image and its segmentation map\n",
    "# pad=True   seg=opt[\"seg\"]\n",
    "ref_image, width, height, segmentation_map  = load_img(opt[\"ref_img\"], SCALE, seg=opt[\"seg\"], target_size=(target_width, target_height))\n",
    "ref_image = repeat(ref_image.to(device), '1 ... -> b ...', b=batch_size)\n",
    "\n",
    "segmentation_map_orig = repeat(torch.tensor(segmentation_map)[None, None, ...].to(device), '1 1 ... -> b 4 ...', b=batch_size)\n",
    "segmentation_map_save = repeat(torch.tensor(segmentation_map)[None, None, ...].to(device), '1 1 ... -> b 3 ...', b=batch_size)\n",
    "segmentation_map = segmentation_map_orig[:, :, ::8, ::8].to(device)\n",
    "\n",
    "top_rr = int((0.5*(target_height - height))/target_height * init_image.shape[2])  # xx% from the top\n",
    "bottom_rr = int((0.5*(target_height + height))/target_height * init_image.shape[2])  \n",
    "left_rr = int((0.5*(target_width - width))/target_width * init_image.shape[3])  # xx% from the left\n",
    "right_rr = int((0.5*(target_width + width))/target_width * init_image.shape[3]) \n",
    "\n",
    "center_row_rm = int(CENTER_ROW_FROM_TOP * target_height)\n",
    "center_col_rm = int(CENTER_COL_FROM_LEFT * target_width)\n",
    "\n",
    "step_height2, remainder = divmod(height, 2)\n",
    "step_height1 = step_height2 + remainder\n",
    "step_width2, remainder = divmod(width, 2)\n",
    "step_width1 = step_width2 + remainder\n",
    "    \n",
    "# compositing in pixel space for same-domain composition\n",
    "save_image[:, :, center_row_rm - step_height1:center_row_rm + step_height2, center_col_rm - step_width1:center_col_rm + step_width2] \\\n",
    "        = save_image[:, :, center_row_rm - step_height1:center_row_rm + step_height2, center_col_rm - step_width1:center_col_rm + step_width2].clone() \\\n",
    "        * (1 - segmentation_map_save[:, :, top_rr:bottom_rr, left_rr:right_rr]) \\\n",
    "        + ref_image[:, :, top_rr:bottom_rr, left_rr:right_rr].clone() \\\n",
    "        * segmentation_map_save[:, :, top_rr:bottom_rr, left_rr:right_rr]\n",
    "\n",
    "# save the mask and the pixel space composited image\n",
    "save_mask = torch.zeros_like(init_image) \n",
    "save_mask[:, :, center_row_rm - step_height1:center_row_rm + step_height2, center_col_rm - step_width1:center_col_rm + step_width2] = 1\n",
    "\n",
    "image = Image.fromarray(((save_image/torch.max(save_image.max(), abs(save_image.min())) + 1) * 127.5)[0].permute(1,2,0).to(dtype=torch.uint8).cpu().numpy())\n",
    "image.save('../outputs/cp_bg_fg.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        for prompts in data:\n",
    "            print(prompts)\n",
    "            c, uc, inv_emb = load_model_and_get_prompt_embedding(model, opt, device, prompts, inv=True)\n",
    "            if not opt[\"cross_domain\"]: # same domain\n",
    "                init_image = save_image\n",
    "            \n",
    "            T1 = time.time()\n",
    "            init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  \n",
    "            \n",
    "            # ref's location in ref image in the latent space\n",
    "            top_rr = int((0.5*(target_height - height))/target_height * init_latent.shape[2])  \n",
    "            bottom_rr = int((0.5*(target_height + height))/target_height * init_latent.shape[2])  \n",
    "            left_rr = int((0.5*(target_width - width))/target_width * init_latent.shape[3])  \n",
    "            right_rr = int((0.5*(target_width + width))/target_width * init_latent.shape[3]) \n",
    "                                    \n",
    "            new_height = bottom_rr - top_rr\n",
    "            new_width = right_rr - left_rr\n",
    "            \n",
    "            step_height2, remainder = divmod(new_height, 2)\n",
    "            step_height1 = step_height2 + remainder\n",
    "            step_width2, remainder = divmod(new_width, 2)\n",
    "            step_width1 = step_width2 + remainder\n",
    "            \n",
    "            center_row_rm = int(CENTER_ROW_FROM_TOP * init_latent.shape[2])\n",
    "            center_col_rm = int(CENTER_COL_FROM_LEFT * init_latent.shape[3])\n",
    "            \n",
    "            param = [max(0, int(center_row_rm - step_height1)), \n",
    "                    min(init_latent.shape[2] - 1, int(center_row_rm + step_height2)),\n",
    "                    max(0, int(center_col_rm - step_width1)), \n",
    "                    min(init_latent.shape[3] - 1, int(center_col_rm + step_width2))]\n",
    "            \n",
    "            ref_latent = model.get_first_stage_encoding(model.encode_first_stage(ref_image))\n",
    "        \n",
    "            shape = [init_latent.shape[1], init_latent.shape[2], init_latent.shape[3]]\n",
    "            z_enc, _ = sampler.sample(steps=opt[\"ddim_steps\"],\n",
    "                                    inv_emb=inv_emb,\n",
    "                                    unconditional_conditioning=uc,\n",
    "                                    conditioning=c,\n",
    "                                    batch_size=opt[\"n_samples\"],\n",
    "                                    shape=shape,\n",
    "                                    verbose=False,\n",
    "                                    unconditional_guidance_scale=opt[\"scale\"],\n",
    "                                    eta=opt[\"ddim_eta\"],\n",
    "                                    order=ORDER,\n",
    "                                    x_T=init_latent,\n",
    "                                    width=width,\n",
    "                                    height=height,\n",
    "                                    DPMencode=True,\n",
    "                                    )\n",
    "            \n",
    "            z_ref_enc, _ = sampler.sample(steps=opt[\"ddim_steps\"],\n",
    "                                        inv_emb=inv_emb,\n",
    "                                        unconditional_conditioning=uc,\n",
    "                                        conditioning=c,\n",
    "                                        batch_size=opt[\"n_samples\"],\n",
    "                                        shape=shape,\n",
    "                                        verbose=False,\n",
    "                                        unconditional_guidance_scale=opt[\"scale\"],\n",
    "                                        eta=opt[\"ddim_eta\"],\n",
    "                                        order=ORDER,\n",
    "                                        x_T=ref_latent,\n",
    "                                        DPMencode=True,\n",
    "                                        width=width,\n",
    "                                        height=height,\n",
    "                                        ref=True,\n",
    "                                        )\n",
    "            \n",
    "            samples_orig = z_enc.clone()\n",
    "\n",
    "            # inpainting in XOR region of M_seg and M_mask\n",
    "            z_enc[:, :, param[0]:param[1], param[2]:param[3]] \\\n",
    "                = z_enc[:, :, param[0]:param[1], param[2]:param[3]] \\\n",
    "                * segmentation_map[:, :, top_rr:bottom_rr, left_rr:right_rr] \\\n",
    "                + torch.randn((1, 4, bottom_rr - top_rr, right_rr - left_rr), device=device) \\\n",
    "                * (1 - segmentation_map[:, :, top_rr:bottom_rr, left_rr:right_rr])\n",
    "\n",
    "            samples_for_cross = samples_orig.clone()\n",
    "            samples_ref = z_ref_enc.clone()\n",
    "            samples = z_enc.clone()\n",
    "\n",
    "            # noise composition\n",
    "            if opt[\"cross_domain\"]: \n",
    "                samples[:, :, param[0]:param[1], param[2]:param[3]] = torch.randn((1, 4, bottom_rr - top_rr, right_rr - left_rr), device=device) \n",
    "                # apply the segmentation mask on the noise\n",
    "                samples[:, :, param[0]:param[1], param[2]:param[3]] \\\n",
    "                        = samples[:, :, param[0]:param[1], param[2]:param[3]].clone() \\\n",
    "                        * (1 - segmentation_map[:, :, top_rr: bottom_rr, left_rr: right_rr]) \\\n",
    "                        + z_ref_enc[:, :, top_rr: bottom_rr, left_rr: right_rr].clone() \\\n",
    "                        * segmentation_map[:, :, top_rr: bottom_rr, left_rr: right_rr]\n",
    "            \n",
    "            mask = torch.zeros_like(z_enc, device=device)\n",
    "            mask[:, :, param[0]:param[1], param[2]:param[3]] = 1\n",
    "                                \n",
    "            samples, _ = sampler.sample(steps=opt[\"ddim_steps\"],\n",
    "                                        inv_emb=inv_emb,\n",
    "                                        conditioning=c,\n",
    "                                        batch_size=opt[\"n_samples\"],\n",
    "                                        shape=shape,\n",
    "                                        verbose=False,\n",
    "                                        unconditional_guidance_scale=opt[\"scale\"],\n",
    "                                        unconditional_conditioning=uc,\n",
    "                                        eta=opt[\"ddim_eta\"],\n",
    "                                        order=ORDER,\n",
    "                                        x_T=[samples_orig, samples.clone(), samples_for_cross, samples_ref, samples, init_latent],\n",
    "                                        width=width,\n",
    "                                        height=height,\n",
    "                                        segmentation_map=segmentation_map,\n",
    "                                        param=param,\n",
    "                                        mask=mask,\n",
    "                                        target_height=target_height, \n",
    "                                        target_width=target_width,\n",
    "                                        center_row_rm=CENTER_ROW_FROM_TOP,\n",
    "                                        center_col_rm=CENTER_COL_FROM_LEFT,\n",
    "                                        tau_a=opt[\"tau_a\"],\n",
    "                                        tau_b=opt[\"tau_b\"],\n",
    "                                        )\n",
    "                \n",
    "            x_samples = model.decode_first_stage(samples)\n",
    "            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "            \n",
    "            T2 = time.time()\n",
    "            print('Running Time: %s s' % ((T2 - T1)))\n",
    "            \n",
    "            for x_sample in x_samples:\n",
    "                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                img.save(os.path.join(sample_path, f\"{base_count:05}_{prompts[0]}.png\"))\n",
    "                base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd2_rebuttal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "262d6cfc02d3adec29b3746e9b87f336431c558f34a822327358e6204ae6159c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
